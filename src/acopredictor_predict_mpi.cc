#include <iostream>
#include <vector>
#include <queue>
#include <cmath>
#include <cstring>
#include <memory>
#include <mpi.h>

#include "acopredictor.h"

/** \file acopredictor_predict_mpi.cc */

using std::cout;
using std::cerr;
using std::vector;
using std::priority_queue;
using std::string;
using std::unique_ptr;
using std::pair;
using std::make_pair;

/** Encapsulates interprocess communication among MPI nodes.
 * This class's best contribution is to hold buffers than can be reused throughout many communications.
 * A strong assumption here is that the machines are homogeneous, so we can pass raw bytes between them.
 */
class InterProcess {
	int dBufSize; /**< Size, in bytes, of recv and send buffers. */
	void *dSendBuffer; /**< Send buffer. */
	void *dRecvBuffer; /**< Receive buffer. */

	/** Serializes the given data within the internal send buffer. */
	void serialize_solution(const vector<char> &directions, int nContacts){
		int nDirections = directions.size();

		// Buffer will contain [nDirections] [nContacts] [direction1, direction2, ..., directionN]
		int *intBuffer = (int*) dSendBuffer;
		*intBuffer = nDirections;
		intBuffer++;
		*intBuffer = nContacts;
		intBuffer++;

		char *charBuffer = (char*) intBuffer;
		memcpy(charBuffer, directions.data(), sizeof(char) * nDirections);
	}

	/** Deserializes data from the internal receive buffer.
	 * \param nContacts Deserialized number of contacts is returned in this variable.
	 * \return Deserialized vector of relative directions.
	 */
	vector<char> deserialize_solution(int &nContacts){
		int *intPointer = (int *) dRecvBuffer;

		int nDirections = *intPointer;
		intPointer++;
		nContacts = *intPointer;
		intPointer++;

		char *charPointer = (char *) intPointer;

		vector<char> directions;
		directions.reserve(nDirections);
		for(int i = 0; i < nDirections; i++){
			directions.push_back(*charPointer);
			charPointer += 1;
		}

		return directions;
	}

public:
	/** Default constructor.
	 * \param numDirections number of relative directions that each protein solution holds.
	 *   This is used to calculate the internal buffer sizes.
	 */
	InterProcess(int numDirections)
	  : dBufSize(2*sizeof(int) + numDirections*sizeof(char)),
	    dSendBuffer((void*) new char[dBufSize]),
		dRecvBuffer((void*) new char[dBufSize])
	{}

	/** Deletes internal buffers.  */
	~InterProcess(){
		delete[] (char*) dSendBuffer;
		delete[] (char*) dRecvBuffer;
	}

	/** Sends a solution to another MPI node.
	 * \param directions
	 * \param contacts
	 * \param destIdx Identifier for the MPI node that should receive the data.
	 */
	void send_solution(const vector<char> &directions, int contacts, int destIdx){
		// Serialize solution into dSendBuffer
		serialize_solution(directions, contacts);

		// Send it
		MPI_Send(dSendBuffer, dBufSize, MPI_CHAR, destIdx, 0, MPI_COMM_WORLD);
	}

	/** Received a solution from another MPI node.
	 * \param directions
	 * \param contacts
	 * \param srcIdx Identifier for the MPI node from which we should receive the data.
	 */
	void recv_solution(vector<char> &directions, int &contacts, int srcIdx){
		// Receive data into dRecvBuffer
		MPI_Recv(dRecvBuffer, dBufSize, MPI_CHAR, srcIdx, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);

		// Deserialize recvBuffer into data
		directions = deserialize_solution(contacts);
	}
};


struct ACOPredictor::Results ACOPredictor::predict(){
	MPI_Init(NULL, NULL);

	int myRank, commSize;
	MPI_Comm_rank(MPI_COMM_WORLD, &myRank);
	MPI_Comm_size(MPI_COMM_WORLD, &commSize);

	// Since we are running multiple processes, we have to guarantee they use different random seeds
	if(dRandSeed < 0){
		std::random_device rd;
		dRandGen[0].seed(rd() + myRank);
	} else {
		dRandGen[0].seed(dRandSeed + myRank);
	}

	if(commSize == 1){
		cerr << "Ran multi-colony program with only one node! This is not allowed.\n";
		exit(EXIT_FAILURE);
	}

	if(dExchangedAnts <= 0){
		cerr << "Ran multi-colony program but specified weird number of EXCHANGED ANTS per cycle. Please check the configuration file.\n";
		exit(EXIT_FAILURE);
	}

	InterProcess interProc(dNMovElems);

	for(int i = 0; i < dCycles; i++){
		vector<ACOSolution> antsSolutions; // Solutions generated by all ants
		unique_ptr<int[]> nContacts(new int[dNAnts]); // Their contacts

		perform_cycle(antsSolutions, nContacts.get());

		// We will need to select the best proteins of current cycle, so we place them all in a priority queue
		auto cmp = [](pair<int,ACOSolution&> a, pair<int,ACOSolution&> b){ return a.first < b.first; };
		priority_queue<pair<int,ACOSolution&>, vector<pair<int,ACOSolution&>>, decltype(cmp)> que(cmp);
		for(unsigned int j = 0; j < antsSolutions.size(); j++)
			que.push(pair<int,ACOSolution&>(nContacts[j], antsSolutions[j]));

		// Exchange N solutions with other colonies
		for(int j = 0; j < dExchangedAnts; j++){
			vector<char> receivedDirections;
			int receivedNContacts;
			vector<char> sendDirections;
			int sendNContacts;

			if(j == 0){
				// On first iteration we exchange globally best solutions
				sendDirections = dBestSol.directions();
				sendNContacts = dBestContacts;
			} else {
				// On other iterations, we exchange best solutions within the pool generated in the current cycle
				pair<int,ACOSolution&> p = que.top();
				que.pop();
				sendDirections = p.second.directions();
				sendNContacts = p.first;
			}

			// Perform ring-exchange
			if(myRank%2 == 0){
				interProc.send_solution(sendDirections, sendNContacts, (myRank+1)%commSize);
				interProc.recv_solution(receivedDirections, receivedNContacts, (myRank-1+commSize)%commSize);
			} else {
				interProc.recv_solution(receivedDirections, receivedNContacts, (myRank-1+commSize)%commSize);
				interProc.send_solution(sendDirections, sendNContacts, (myRank+1)%commSize);
			}

			// Check best sol
			if(myRank == 0 && receivedNContacts > dBestContacts){
				dBestSol = ACOSolution(receivedDirections);
				dBestContacts = receivedNContacts;
			}

			//cout << "I am " << myRank << ", sent " << sendNContacts << " and received " << receivedNContacts << "\n";

			// Deposit pheromones for received ant
			ant_deposit_pheromone(receivedDirections, receivedNContacts);
		}

		/*
		if(myRank == 0){
			cout << receivedNContacts << ": ";
			for(char c: receivedDirections){
				cout << (int) c << " ";
			} cout << "\n";
		}
		*/

		/*
		if(i%2 == 0){
			for(int j = 0; j < 5; j++){
				for(int i = 0; i < dNMovElems; i++)
					cout << pheromone(i, j) << " ";
				cout << "\n";
			}
			cout << "\n";
		}
		*/
	}

	// Gather solutions in node 0
	if(myRank == 0){
		vector<char> receivedDirections;
		int receivedNContacts;

		//cout << "Solution from node 0 is " << dBestContacts << ".\n";

		// Check which one is best
		for(int i = 1; i < commSize; i++){
			interProc.recv_solution(receivedDirections, receivedNContacts, i);
			//cout << "Solution from node " << i << " is " << receivedNContacts << ".\n";
			if(receivedNContacts > dBestContacts){
				dBestSol = ACOSolution(receivedDirections);
				dBestContacts = receivedNContacts;
			}
		}
	} else {
		interProc.send_solution(dBestSol.directions(), dBestContacts, 0);
	}

	Results res = {
		.solution = dBestSol,
		.contacts = dBestContacts
	};

	MPI_Finalize();

	if(myRank != 0){
		exit(EXIT_SUCCESS);
	}

	return res;
}
